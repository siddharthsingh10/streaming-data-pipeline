# Streaming Pipeline Design Document

## Overview

A real-time streaming pipeline that processes user events through Kafka, with comprehensive error handling, dead letter queue management, and observability features.

## Architecture

### Core Components

1. **Event Producer** (`src/producer.py`)
   - Generates user events (valid and invalid for testing)
   - Validates events against schema
   - Routes valid events to `events-topic`
   - Routes invalid events to `dead-letter-topic`

2. **Event Consumer** (`src/consumer.py`)
   - Consumes valid events from `events-topic`
   - Transforms events and writes to Parquet files
   - Handles batch processing for efficiency

3. **Dead Letter Consumer** (`src/consumer.py`)
   - Consumes invalid events from `dead-letter-topic`
   - Writes dead letter events as JSON files for debugging
   - No further processing to avoid loops

4. **Pipeline Orchestrator** (`src/pipeline.py`)
   - Coordinates all components
   - Manages parallel processing threads
   - Provides comprehensive metrics and health monitoring

5. **Data Writers**
   - **ParquetSinkWriter**: Writes valid events to Parquet files
   - **DeadLetterSinkWriter**: Writes invalid events to JSON files

### Data Flow

```
Producer → Kafka Topics → Consumers → Data Storage
    ↓           ↓              ↓           ↓
Generate   Route by     Process in    Write to
Events     Validity     Parallel      Files
```

### Kafka Topics

- **`events-topic`**: Valid events for processing
- **`dead-letter-topic`**: Invalid events for debugging

## Key Features

### 1. Dead Letter Queue Management
- **Producer-level validation**: Invalid events sent to dead letter topic
- **Consumer separation**: Valid and dead letter events processed separately
- **No processing loops**: Dead letter events written as JSON and forgotten
- **Error categorization**: Automatic error analysis and categorization

### 2. Parallel Processing
- **Multi-threaded architecture**: Producer, events consumer, and dead letter consumer run in parallel
- **Independent processing**: Each component operates independently
- **Graceful shutdown**: Proper cleanup and resource management

### 3. Comprehensive Metrics
- **Separate tracking**: Valid events vs dead letter events
- **Real-time monitoring**: Health checks and performance metrics
- **Success/error rates**: Calculated based on valid events only
- **Processing statistics**: Events per second, batch processing stats

### 4. Error Handling
- **Schema validation**: Events validated against JSON schema
- **Graceful degradation**: Failed events don't stop the pipeline
- **Error categorization**: Automatic classification of error types
- **Retry logic**: Configurable retry mechanisms

## Configuration

### Kafka Configuration
```python
KAFKA_CONFIG = {
    'bootstrap.servers': 'localhost:29092',
    'client.id': 'streaming-pipeline',
    'auto.offset.reset': 'earliest',
    'enable.auto.commit': True,  # Auto-commit offsets
    'group.id': 'streaming-pipeline-group'
}
```

### Consumer Configuration
```python
CONSUMER_CONFIG = {
    'enable.auto.commit': True,    # Auto-commit offsets
    'auto.offset.reset': 'latest', # Only read new messages
    'session.timeout.ms': 30000,
    'heartbeat.interval.ms': 3000
}
```

## Metrics and Monitoring

### Pipeline Metrics
- **Events Produced**: Total events generated by producer
- **Valid Events Consumed**: Events from `events-topic`
- **Dead Letter Events Consumed**: Events from `dead-letter-topic`
- **Events Written**: Successfully written to Parquet files
- **Success Rate**: Percentage of valid events successfully processed
- **Error Rate**: Percentage of valid events that failed processing

### Health Monitoring
- **Component health**: Individual health checks for each component
- **Overall status**: Aggregated health status
- **Real-time alerts**: Warning and error notifications
- **Performance metrics**: Processing rates and throughput

## Error Handling Strategy

### 1. Producer Errors
- Invalid events sent to dead letter topic
- Error categorization and analysis
- JSON files created for debugging

### 2. Consumer Errors
- Failed transformations logged
- Dead letter events created for processing errors
- Graceful error recovery

### 3. Dead Letter Processing
- Simple JSON file writing
- No further processing to avoid loops
- Error analysis and categorization

## Performance Characteristics

### Throughput
- **Target**: 10 events/second (configurable)
- **Batch processing**: 100 events per batch
- **Parallel processing**: Multiple threads for scalability

### Latency
- **End-to-end**: < 1 second for valid events
- **Dead letter processing**: Immediate JSON file writing
- **Batch processing**: Configurable batch sizes and timeouts

### Reliability
- **At-least-once delivery**: Kafka guarantees
- **Error recovery**: Graceful handling of failures
- **Data persistence**: Parquet and JSON file storage

## Deployment

### Prerequisites
- Docker and Docker Compose
- Python 3.8+
- Kafka cluster (provided via Docker Compose)

### Running the Pipeline
```bash
# Start Kafka infrastructure
docker-compose up -d

# Run the pipeline
python3 main.py

# Run demo
python3 demo_mvp.py
```

### Configuration
- **Duration**: Configurable runtime (default: 300 seconds)
- **Event rate**: Configurable events per second (default: 10)
- **Invalid event ratio**: Configurable percentage (default: 5%)

## Recent Fixes and Improvements

### 1. Dead Letter Processing Loop Fix
- **Issue**: Dead letter consumer was creating infinite processing loops
- **Solution**: Dead letter events are now simply written as JSON files without further processing
- **Result**: Clean separation between valid and invalid event processing

### 2. Consumer Offset Management
- **Issue**: Consumer was reading old events from previous runs
- **Solution**: Enabled auto-commit and set offset reset to 'latest'
- **Result**: Only new events are processed, accurate metrics

### 3. Metrics Separation
- **Issue**: Metrics were mixing valid and dead letter events
- **Solution**: Separate tracking for valid events vs dead letter events
- **Result**: Clear visibility into processing success rates

### 4. Topic Cleanup
- **Issue**: Multiple topics created during development
- **Solution**: Clean slate approach with proper topic management
- **Result**: Clean, predictable topic structure

## Future Enhancements

1. **Partitioning Strategy**: Key-based partitioning for sequential processing
2. **Consumer Groups**: Multiple consumer instances for scalability
3. **Monitoring Integration**: Prometheus metrics and Grafana dashboards
4. **Alerting**: Automated alerts for pipeline issues
5. **Data Retention**: Configurable data retention policies 