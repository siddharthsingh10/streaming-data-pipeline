# Streaming Data Ingestion Pipeline

A minimal yet complete streaming data pipeline that ingests JSON events via Kafka, applies meaningful transformations, and writes output to structured sinks with error handling.

## ğŸ— Architecture

```
Producer â†’ Kafka â†’ Consumer â†’ Transform â†’ Parquet Sink
                â†“
            Dead Letter Queue
```

## ğŸš€ Quick Start

### Prerequisites
- Docker and Docker Compose
- Python 3.8+

### Setup

1. **Start Kafka Infrastructure**
   ```bash
   docker-compose up -d
   ```

2. **Install Dependencies**
   ```bash
   pip install -r requirements.txt
   ```

3. **Run the Pipeline**
   ```bash
   python main.py
   ```

## ğŸ“ Project Structure

```
streaming_pipeline/
â”œâ”€â”€ src/                    # Core application code
â”œâ”€â”€ schema/                 # Data schemas and validation
â”œâ”€â”€ data/                   # Output data storage
â”‚   â”œâ”€â”€ output/            # Processed data (Parquet)
â”‚   â””â”€â”€ dead/              # Failed events
â”œâ”€â”€ logs/                  # Application logs
â”œâ”€â”€ tests/                 # Unit tests
â””â”€â”€ docker-compose.yml     # Kafka infrastructure
```

## ğŸ”§ Components

- **Producer**: Generates synthetic events with validation
- **Consumer**: Processes events with transformations
- **Dead Letter Queue**: Handles failed events
- **Schema Validation**: Language-agnostic validation using JSON Schema
- **Parquet Sink**: Efficient data storage

## ğŸ“Š Features

- âœ… Streaming data source simulation (Kafka-based)
- âœ… Meaningful transformations (normalization, enrichment)
- âœ… Structured sink storage (Parquet format)
- âœ… Error handling with dead letter queue
- âœ… Batch processing for efficiency
- âœ… Comprehensive test coverage
- âœ… Runnable end-to-end pipeline

## ğŸ§ª Testing

```bash
pytest tests/
```

## ğŸ“ Design Decisions

See `DESIGN_DOCUMENT.md` for detailed architecture and design rationale.

## ğŸš€ Quick Demo

Run the minimal pipeline:

```bash
# Start infrastructure
docker-compose up -d

# Install dependencies
pip install -r requirements.txt

# Run the pipeline
python main.py
```

This will run a 60-second demo with 5 events per second, demonstrating the complete streaming pipeline. 